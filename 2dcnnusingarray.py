# -*- coding: utf-8 -*-
"""2DCNNusingArray.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ygwEAVHIFSnLvnRA6N_SE63iTWWRXgAK
"""

from google.colab import drive
drive.mount('/content/drive')

!gdown --id 1Lx4s240DQDVIF2Zz7P4_DpuBKu8ELUrY

!ls /content/TestData.zip -al

import zipfile
from google.colab import drive

drive.mount('/content/drive')

!unzip "/content/TestData.zip" -d "/content/TestData"

# coding:utf-8
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import re
import os
from PIL import Image
import glob
import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, LSTM
from tensorflow.keras.preprocessing.image import ImageDataGenerator

csvdata_dir = "/content/TestData/TestData/"

# raw data from the sensor
folder = ["GrippingLeftFaster", "GrippingLeft",  "GrippingOnly", "GrippingWithDiffForce"]


max_dataarraysize = 30052

#df maximum value
#last_line = len(folder[0].iloc[:,-1:])

data_collect = []
label = []

n = 0
k = 10

for index, name in enumerate(folder):
    name = name + "/csvs"
    #print(name)
    dir = csvdata_dir + name
    files = glob.glob(dir + "/*.csv")

    for i, file in enumerate(files):
        csv = pd.read_csv(file, sep = ',', encoding = "UTF-8", error_bad_lines = False, header = None)
        data = np.array(csv, dtype = np.float32)
        if csv.empty:
            print('DataFrame is empty!')        # (198959, 3)
        print(data.shape)

        # (19896, 3)
        for i in range(int(max_dataarraysize)):
          

          #For data_collect shape to be (14000, 3, 10)
          preStack = np.vstack((data[n:k, 0], data[n:k, 1]))
          data_stack= np.vstack((preStack, data[n:k, 2]))
          
          data_collect.append(data_stack)
          #print(data_collect)
          label.append([index])
          #print(label)
          n = n+10
          k = k+10
          #print(n,k)

        n = 0
        k = 10

        #data_collect.append(data_collect)

#data_collect = np.asarray(data_stack)

data_collect = np.asarray(data_collect)
label = np.asarray(label)
label =  tf.keras.utils.to_categorical(label, 4)

print(data_collect.shape)
print(label.shape)
# (14000, 30)

X_train, X_test, y_train, y_test = train_test_split(data_collect, label, test_size=0.2)


'''for debugging'''
'''
# (828, 2300, 3)
print(data_collect.shape)
# (2300, 3)
print(data_collect[1])
print(data_collect[2])
print(data_collect[3])

print(type(data_collect))
'''

print(type(X_test), type(X_train), type(y_train), type(y_test))

print((X_test.shape))
print((X_train.shape))
print(y_test.shape)

X_test = np.array([np.array(val) for val in X_test])
X_train = np.array([np.array(val) for val in X_train])
y_test = np.array([np.array(val) for val in y_test])
y_train = np.array([np.array(val) for val in y_train])

X_train = X_train / 255.0
X_test = X_test / 255.0

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

#necessary libraries, functions, layers, etc.
from sklearn.metrics import classification_report, precision_recall_fscore_support
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.inception_v3 import preprocess_input
from keras.applications.inception_v3 import decode_predictions
from keras.applications.inception_v3 import InceptionV3
from sklearn.model_selection import train_test_split
from IPython.display import Image
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten
from os import listdir

# Create model
cnn = models.Sequential([
    # Using 32 filters(understand it as a depth)
    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(3, 10, 1)),
    # Max pooling is used to redue the spatial dimensions
    # Pool size of (2,2)
    #layers.MaxPooling2D((2, 2)),
    # Using 64 filters(understand it as a depth)
    # As the output spatial volume is decreasing, 
    # the number of filters learned is increasing
    layers.Conv2D(filters=32, kernel_size=(1, 1), activation='relu'),
    # Max pooling is used to redue the spatial dimensions
    #layers.MaxPooling2D((1, 1)),
    # 'Flatten' converts matrix to single array
    layers.Flatten(),
    # 'Dense' is the actual network layer in the model
    # 32 neurons
    layers.Dense(32, activation='relu'),
    # 4 neurons
    layers.Dense(4, activation='softmax')
])

cnn.summary()

cnn.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint("model_weights.h5", monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

# Fit the model
history = cnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)

# serialize model structure to JSON
model_json = cnn.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix

y_pred=cnn.predict(X_test)

print(y_test.shape)
print(y_pred.shape)

y_pred=np.argmax(y_pred, axis=1)
y_test=np.argmax(y_test, axis=1)
cm = confusion_matrix(y_test, y_pred)
print(cm)

#cm = confusion_matrix(y_test, y_pred)
labels = ["GrippingLeftFaster", "GrippingLeft", "GrippingOnly","GrippingWIthDiffForce"]
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)

disp.plot(cmap=plt.cm.Blues)
plt.show()

"""Saving our model in SavedModel format"""

from tensorflow.keras.models import Sequential, save_model, load_model

# Save the model
filepath = './saved_model'
save_model(model, filepath)

"""Loading the model"""

# Load the model
model = load_model(filepath, compile = True)



from keras.models import model_from_json
import numpy as np

class FacialExpressionModel(object):

    EMOTIONS_LIST = ["DiffForceLeft", "GrippingLeft",
                     "GrippingOnly", "GrippingWithDiffForce"]

    def __init__(self, model_json_file, model_weights_file):
        # load model from JSON file
        with open(model_json_file, "r") as json_file:
            loaded_model_json = json_file.read()
            self.loaded_model = model_from_json(loaded_model_json)

        # load weights into the new model
        self.loaded_model.load_weights(model_weights_file)
        self.loaded_model._make_predict_function()

    def predict_emotion(self, img):
        self.preds = self.loaded_model.predict(img)
        return FacialExpressionModel.EMOTIONS_LIST[np.argmax(self.preds)]

"""
*   get the image stream from our webcam
*   detect faces with OpenCV and add bounding boxes
*   convert the faces to greyscale, rescale them and send them to our pre-trained Neural Network
*   get the predictions back from our Neural Network and *   add the label to the webcam image
*   return the final image stream

"""

import cv2
from model import FacialExpressionModel
import numpy as np

facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
model = FacialExpressionModel("model.json", "model_weights.h5")
font = cv2.FONT_HERSHEY_SIMPLEX

class VideoCamera(object):

    # returns camera frames along with bounding boxes and predictions
    def get_frame(self):
        _, fr = self.video.read()
        gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)
        faces = facec.detectMultiScale(gray_fr, 1.3, 5)

        for (x, y, w, h) in faces:
            fc = gray_fr[y:y+h, x:x+w]

            roi = cv2.resize(fc, (48, 48))
            pred = model.predict_emotion(roi[np.newaxis, :, :, np.newaxis])

            cv2.putText(fr, pred, (x, y), font, 1, (255, 255, 0), 2)
            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)

        _, jpeg = cv2.imencode('.jpg', fr)
        return jpeg.tobytes()